---
title: "p8105_hw5_nc2710"
author: "Nicole Comfort"
date: "11/8/2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

devtools::install_github("thomasp85/patchwork")

library(tidyverse)
library(ggplot2)
library(ggridges)
library(readxl)
library(dplyr)
library(janitor)
library(patchwork)
library(viridis)
library(rvest)
library(purrr)
library(stringr) 

# set options for figures
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "95%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Problem 1 

The hw5_data zip file contains data from a longitudinal study that included a control arm and an experimental arm. Data for each participant is included in a separate file, and file names include the subject ID and arm (10 subjects per arm). 

The code below creates a tidy dataframe containing data from all participants, including the subject ID, arm, and observations over time (8 weeks): 

```{r import and tidy data}

# create vector of file names to load using the list.files function 
file_name <- list.files(path = "./data") %>% 
  purrr::set_names()

# Iterate over file names and read in data for each subject using purrr::map and saving the result as a new variable in the dataframe
# Tidy the result; manipulate file names to include control arm and subject ID
import_data = function(x) {
  
  df = read_csv(file = str_c("./data/", x)) %>% 
    mutate(filename = x) %>% # adds col with file name
    separate(filename, into = c("file", "remove"), sep = "\\.") %>% # remove .csv from file col
    select(-remove) %>% 
    separate(file, into = c("arm", "subject_id"), sep = "_") # separate file col into arm and subj id variables
  df
  
}

# Run the load_data function on all file names saved in the filename_vector, save output as a list col in a new df called final_df
final_df = 
  tibble(file_name) %>% 
    mutate(data = map(file_name, import_data)) %>% 
      unnest()

# Make sure weekly observations are “tidy” (i.e. long format) and do any other tidying that’s necessary
final_df =
  final_df %>% 
    gather(key = week, value = observation, week_1:week_8) %>% 
    separate(week, into = c("remove", "week"), sep = "_") %>%
    select(-remove) %>% 
    mutate(arm = str_replace(arm, "con", "Control"),
           arm = str_replace(arm, "exp", "Experimental")) %>% 
    mutate(week = as.numeric(week))

```

The raw data includes 20 files (one for each subject with 10 control subjects and 10 experimental subjects), each with eight weeks of observations. I changed this from wide to long format. This results in a final dataset (named final_df) that includes `r nrow(final_df)` observations of `r ncol(final_df)` variables, including: the filename, study arm, subject ID, week of observation, and value of the observation recorded. 

The code below creates a spaghetti plot showing observations of each subject over time:

```{r spaghetti plot}

final_df %>% 
  ggplot(aes(x = week, y = observation, color = subject_id)) +
  geom_line() +
  facet_grid(~arm) +
  labs(
    title = "Observations Across Weeks by Study Arm",
    x = "Week",
    y = "Observation Value"
  )

```

Looking at the spaghetti plot, it is apparent that subjects in the experimental arm had increased values over time compared to subjects in the control group, whose observations seemed to remain relatively stable over time. Also, it looks like the subjects allocated to the experimental group had greater variability in their baseline values than the control subjects.  

## Problem 2 

The Washington Post has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository. The code below loads the raw data: 

```{r import homicide data}

homicide_data =
  read_csv(file = "./homicide-data/homicide-data.csv") # import dataset

homicide_data %>% 
  distinct(state)

min(homicide_data$reported_date)
max(homicide_data$reported_date)

```

The raw data is a dataframe regarding homicides from 28 different states that took place from Jan 1, 2007 to November 2015. The dataset is composed of `r nrow(homicide_data)` observations of `r ncol(homicide_data)` variables. The variables include: unique case ID, reported date, victim last name and first name, as well as demographic information of the victim (race, age, sex). There is also information on the location including city, state, latitude/longitude, and finally, the disposition of the case (e.g. whether or not the case was resolved). 

We will perform some data manipulation to create a new variable indicating both city and state, and for each city/state, obtain the total number of homicides as well as the number of unsolved homicides: 

```{r data manipulation}

homicide_data = 
  homicide_data %>% 
  janitor::clean_names() %>% # clean names
  mutate(city_state = str_c(city, state, sep = ", ", collapse = NULL)) %>% # create a city_state variable (e.g. “Baltimore, MD”)
  group_by(city_state, disposition) %>% 
  summarize(num_homicides = n()) %>% 
  summarize(total_homicides = sum(num_homicides), # summarize within cities to obtain total # of homicides
            unsolved_homicides = sum(num_homicides[disposition == "Closed without arrest" | disposition == "Open/No arrest"])) # summarize within cities to obtain # of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```

Using this new dataset, for each location, we can look at the proportion of homicides that were unresolved (i.e. case not closed by arrest). 

Over this time period, Tulsa, AZ had the lowest number of total homicides, with only 1, whereas Chicago had the most total homicides, with 5535 cases. 

We will next estimate the proportion of homicides that are unresolved for the city of Baltimore, MD: 

```{r Baltimore prop.test}

# For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved

# Manually, we can do the math to look at the proportion of unresolved homicides for Baltimore: 1825/2877 = 0.64556 so this will be the estimate we try to get our code to produce 

prop_test_df = 
  homicide_data %>% 
  filter(city_state == "Baltimore, MD")
  
baltimore_prop_test = # Save the output of prop.test as an R object
  prop.test(prop_test_df$unsolved_homicides, prop_test_df$total_homicides,
            conf.level = 0.95, correct = TRUE) %>% 
  broom::tidy(baltimore_prop_test) %>% # Apply the broom::tidy to this object 
  select(estimate, conf.low, conf.high) %>% # Pull the estimated proportion and confidence intervals from the resulting tidy dataframe
  knitr::kable(digits = 4)

print(baltimore_prop_test)
# Indeed, our two-sided prop.test gave us an estimate of 0.64556, as expected

```

The proportion of homicides in Baltimore, MD that are unresolved is 0.6456. We are 95% confident that the "true" estimate lies within the range (0.6276, 0.6632). That means that more than half of the homicides in Baltimore remain unresolved. 

We will next run this test for each of the cities in the dataset: 

```{r unsolved homicides all cities}

# Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

prop_test_fxn = function(x) {
  
  test_result =
        broom::tidy(
          prop.test(x = homicide_data[["unsolved_homicides"]][[x]], homicide_data[["total_homicides"]][[x]])) %>%
          select(estimate, conf.low, conf.high)
  test_result
}

# Run the prop. test on each row, save results to a list column in a new df

homicide_data_prop_tests = homicide_data %>%
  mutate(prop_test = map(1:nrow(homicide_data), prop_test_fxn)) %>% 
  unnest() %>% 
  janitor::clean_names()

# Warning: Chi-squared approximation may be incorrect.? Should I apply a continuity correction? 

```

The city with the largest proportion of unsolved homicides is Chicago, IL followed by New Orleans, LA then Baltimore, MD. The city with the lowest proportion of unsolved homicides is Tulsa, AL for which there was only 1 homicide which was resolved. After Tulsa, AL the cities with the lowest estimates for unsolved homicides is Richmond, VA then Charlotte, NC. 

Below is a plot that shows the estimates for unsolved homicides and corresponding 95% confidence intervals for each city:

```{r homicide plot}

homicide_data_prop_tests %>% 
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) + # Organize cities according to the proportion of unsolved homicides 
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) + # add error bars based on upper/lower CI limits 
    labs(
      title = "Unsolved Homicide Cases by City, 2007-2015",
      x = "City",
      y = "Proportion of Unsolved Homicides",
      caption = "Data obtained from the Washington Post"
    ) + 
  theme(axis.text.x = element_text(angle = 75, hjust = 1))

```

From the plot, we can see that Tulsa, AL has an extremely wide confidence interval, but this is due to the very limited sample size (only one homicide case). Looking into this 'outlier' further, it appears that this entry was likely in error and that this data point belongs to Tulsa, OK, because the Washington Post dataset says that the data describes homicides that took place in 50 cities yet the dataset lists 51 cities when we include the city_state variable, with two rows for Tulsa. 

```{r remove Tulsa, AL}

# Remove the row for Tulsa, AL
homicide_data_prop_tests =
  filter(homicide_data_prop_tests, city_state != "Tulsa, AL")

# Re-plot excluding the Tulsa, AL 
homicide_data_prop_tests %>% 
  ggplot(aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
  labs(
    title = "Unsolved Homicide Cases by City, 2007-2015",
    x = "City",
    y = "Proportion of Unsolved Homicides",
    caption = "Data obtained from the Washington Post"
    ) + 
  theme(axis.text.x = element_text(angle = 75, hjust = 1))
# note that this estimate will still be slightly off because we are one homicide case short 

```

When we remove the incorrect Tulsa, AL entry, we can see that Richmond, VA has the lowest proportion of unsolved homicides, followed by Charlotte, NC then Memphis, TN and Tulsa, OK. Note that the estimate for Tulsa, OK is not completely accurate because it is one homicide case short, but with 583 total homicides I doubt that one additional case would impact the confidence intervals that much. 

Chicago, IL has the smallest confidence interval because it has the largest sample size of 5,535 total homicides.

It would be interesting to look at how these unsolved homicide cases vary by demographics of the victim (sex, age, race) as well as how the number of unsolved homicide cases changes over time. I would hope that with each passing year, a larger proportion of homicide cases become resolved by arrest. 